{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92834ac-3dd5-45a5-8af1-a479f4cc3faa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1090485470240688#setting/sparkui/0725-092639-ps8v7iqi/driver-2793214379506251747\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.6:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa3b05eeec0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d5221b-0a9a-4d07-b19c-64d972e4217a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = 'azuresynaptraining'\n",
    "storage_account_access_key = '=='\n",
    "spark.conf.set('.' + storage_account_name + '.blob.core.windows.net', storage_account_access_key)\n",
    "blob_container = 'task'\n",
    "filePath = f\"\" + blob_container + \"@\" + storage_account_name + \".\"\n",
    "zomato_df = spark.read.format(\"csv\").load(filePath, inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76b8bf17-9bb3-4859-bf23-be898347d0e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+----------------+--------+-----+----------+\n",
      "|OrderID|CustomerID|RestaurantName|        DishName|Quantity|Price| OrderDate|\n",
      "+-------+----------+--------------+----------------+--------+-----+----------+\n",
      "|      1|       183|     Spice Hub|    Paneer Tikka|       3|  322|2024-06-04|\n",
      "|      2|       186|   Sushi World|    Cheeseburger|       5|  466|2024-06-23|\n",
      "|      3|       127|   Pasta Place|    Cheeseburger|       5|  260|2024-06-08|\n",
      "|      4|       145|  Pizza Palace|  Mango Smoothie|       2|  382|2024-06-26|\n",
      "|      5|       144|  Pizza Palace| Pepperoni Pizza|       1|  154|2024-06-26|\n",
      "|      6|       110|  Pizza Palace|    Paneer Tikka|       1|  411|2024-06-18|\n",
      "|      7|       156|   Pasta Place|    Cheeseburger|       1|  110|2024-06-05|\n",
      "|      8|       126|   Sushi World|  Veggie Delight|       5|  494|2024-06-05|\n",
      "|      9|       108|  Burger Haven| Pepperoni Pizza|       3|  474|2024-06-02|\n",
      "|     10|       119|   Pasta Place| Pepperoni Pizza|       3|  136|2024-06-14|\n",
      "|     11|       104|     Spice Hub| California Roll|       5|  421|2024-06-22|\n",
      "|     12|       198|   Sushi World| Chicken Biryani|       2|  194|2024-06-09|\n",
      "|     13|       177|  Pizza Palace|  Mango Smoothie|       5|  138|2024-06-01|\n",
      "|     14|       113|  Pizza Palace|   Chicken Curry|       3|  380|2024-06-07|\n",
      "|     15|       108|  Pizza Palace|    Paneer Tikka|       2|  301|2024-06-22|\n",
      "|     16|       129|  Pizza Palace| Chicken Biryani|       3|  110|2024-06-20|\n",
      "|     17|       157|  Burger Haven| California Roll|       2|  392|2024-06-22|\n",
      "|     18|       186|     Spice Hub|Margherita Pizza|       1|  194|2024-06-16|\n",
      "|     19|       171|     Spice Hub|   Chicken Curry|       5|  372|2024-06-25|\n",
      "|     20|       131|   Sushi World|  Veggie Delight|       2|  150|2024-06-03|\n",
      "+-------+----------+--------------+----------------+--------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zomato_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ffb49f0-5d1d-48ae-931f-89d79f432941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "zomato_df.createOrReplaceTempView('zomato_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81fab29-733e-428d-8a21-debb136732e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, current_timestamp, window, col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "zomato_stream = spark.readStream\\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(zomato_df.schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(filePath)\\\n",
    "    .withColumn('timestamp', current_timestamp().cast(TimestampType()))\n",
    "\n",
    "zomato_stream = zomato_stream.withWatermark(\"timestamp\", \"5 minutes\")\n",
    "\n",
    "\n",
    "zomato_stream.createOrReplaceTempView('zomato_db')\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "        SELECT RestaurantName, AVG(CustomerOrderCount) AS AvgOrdersPerCustomer,timestamp\n",
    "        FROM (\n",
    "            SELECT RestaurantName, CustomerID, COUNT(OrderID) AS CustomerOrderCount,timestamp\n",
    "            FROM zomato_db\n",
    "            GROUP BY RestaurantName, CustomerID, timestamp\n",
    "        ) AS CustomerOrderCounts\n",
    "        GROUP BY RestaurantName, timestamp\n",
    "        ORDER BY AvgOrdersPerCustomer DESC\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4e7023-16c5-4c58-9576-695a1372af9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-2640412931704828>, line 25\u001b[0m\n",
       "\u001b[1;32m     14\u001b[0m     batch_df\u001b[38;5;241m.\u001b[39mwrite \\\n",
       "\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosmos.oltp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
       "\u001b[1;32m     16\u001b[0m         \u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig) \\\n",
       "\u001b[1;32m     17\u001b[0m         \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
       "\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m.\u001b[39msave()\n",
       "\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Write the streaming data to Cosmos DB using foreachBatch\u001b[39;00m\n",
       "\u001b[1;32m     21\u001b[0m streaming_query \u001b[38;5;241m=\u001b[39m result_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n",
       "\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(write_to_cosmos) \\\n",
       "\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
       "\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/azuredatabricks/default/checkpoint2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
       "\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
       "\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Wait for the streaming query to finish\u001b[39;00m\n",
       "\u001b[1;32m     28\u001b[0m streaming_query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py:1637\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n",
       "\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n",
       "\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[0;32m-> 1637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    226\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: Detected pattern of possible 'correctness' issue due to global watermark. The query contains stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are \"late rows\" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details. If you understand the possible risk of correctness issue and still need to run the query, you can disable this check by setting the config `spark.sql.streaming.statefulOperator.checkCorrectness.enabled` to false.; line 9 pos 8;\n",
       "Sort [AvgOrdersPerCustomer#698 DESC NULLS LAST], true\n",
       "+- Aggregate [RestaurantName#675, timestamp#687-T300000ms], [RestaurantName#675, avg(CustomerOrderCount#697L) AS AvgOrdersPerCustomer#698, timestamp#687-T300000ms]\n",
       "   +- SubqueryAlias CustomerOrderCounts\n",
       "      +- Aggregate [RestaurantName#675, CustomerID#674, timestamp#687-T300000ms], [RestaurantName#675, CustomerID#674, count(OrderID#673) AS CustomerOrderCount#697L, timestamp#687-T300000ms]\n",
       "         +- SubqueryAlias zomato_db\n",
       "            +- View (`zomato_db`, [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679, timestamp#687-T300000ms])\n",
       "               +- EventTimeWatermark timestamp#687: timestamp, 5 minutes\n",
       "                  +- Project [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679, cast(current_timestamp() as timestamp) AS timestamp#687]\n",
       "                     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1b8005c8,csv,List(),Some(StructType(StructField(OrderID,IntegerType,true),StructField(CustomerID,IntegerType,true),StructField(RestaurantName,StringType,true),StructField(DishName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true),StructField(OrderDate,DateType,true))),List(),None,Map(header -> true, path -> wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv),None), FileSource[wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv], [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "Detected pattern of possible 'correctness' issue due to global watermark. The query contains stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are \"late rows\" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details. If you understand the possible risk of correctness issue and still need to run the query, you can disable this check by setting the config `spark.sql.streaming.statefulOperator.checkCorrectness.enabled` to false.; line 9 pos 8;\nSort [AvgOrdersPerCustomer#698 DESC NULLS LAST], true\n+- Aggregate [RestaurantName#675, timestamp#687-T300000ms], [RestaurantName#675, avg(CustomerOrderCount#697L) AS AvgOrdersPerCustomer#698, timestamp#687-T300000ms]\n   +- SubqueryAlias CustomerOrderCounts\n      +- Aggregate [RestaurantName#675, CustomerID#674, timestamp#687-T300000ms], [RestaurantName#675, CustomerID#674, count(OrderID#673) AS CustomerOrderCount#697L, timestamp#687-T300000ms]\n         +- SubqueryAlias zomato_db\n            +- View (`zomato_db`, [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679, timestamp#687-T300000ms])\n               +- EventTimeWatermark timestamp#687: timestamp, 5 minutes\n                  +- Project [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679, cast(current_timestamp() as timestamp) AS timestamp#687]\n                     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1b8005c8,csv,List(),Some(StructType(StructField(OrderID,IntegerType,true),StructField(CustomerID,IntegerType,true),StructField(RestaurantName,StringType,true),StructField(DishName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true),StructField(OrderDate,DateType,true))),List(),None,Map(header -> true, path -> wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv),None), FileSource[wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv], [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679]\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Detected pattern of possible 'correctness' issue due to global watermark. The query contains stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are \"late rows\" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details. If you understand the possible risk of correctness issue and still need to run the query, you can disable this check by setting the config `spark.sql.streaming.statefulOperator.checkCorrectness.enabled` to false.; line 9 pos 8;\nSort [AvgOrdersPerCustomer#698 DESC NULLS LAST], true\n+- Aggregate [RestaurantName#675, timestamp#687-T300000ms], [RestaurantName#675, avg(CustomerOrderCount#697L) AS AvgOrdersPerCustomer#698, timestamp#687-T300000ms]\n   +- SubqueryAlias CustomerOrderCounts\n      +- Aggregate [RestaurantName#675, CustomerID#674, timestamp#687-T300000ms], [RestaurantName#675, CustomerID#674, count(OrderID#673) AS CustomerOrderCount#697L, timestamp#687-T300000ms]\n         +- SubqueryAlias zomato_db\n            +- View (`zomato_db`, [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679, timestamp#687-T300000ms])\n               +- EventTimeWatermark timestamp#687: timestamp, 5 minutes\n                  +- Project [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679, cast(current_timestamp() as timestamp) AS timestamp#687]\n                     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1b8005c8,csv,List(),Some(StructType(StructField(OrderID,IntegerType,true),StructField(CustomerID,IntegerType,true),StructField(RestaurantName,StringType,true),StructField(DishName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true),StructField(OrderDate,DateType,true))),List(),None,Map(header -> true, path -> wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv),None), FileSource[wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv], [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679]\n"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": null,
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m<command-2640412931704828>, line 25\u001b[0m\n\u001b[1;32m     14\u001b[0m     batch_df\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosmos.oltp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig) \\\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Write the streaming data to Cosmos DB using foreachBatch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m streaming_query \u001b[38;5;241m=\u001b[39m result_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(write_to_cosmos) \\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/azuredatabricks/default/checkpoint2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Wait for the streaming query to finish\u001b[39;00m\n\u001b[1;32m     28\u001b[0m streaming_query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py:1637\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
        "\u001b[0;31mAnalysisException\u001b[0m: Detected pattern of possible 'correctness' issue due to global watermark. The query contains stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are \"late rows\" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details. If you understand the possible risk of correctness issue and still need to run the query, you can disable this check by setting the config `spark.sql.streaming.statefulOperator.checkCorrectness.enabled` to false.; line 9 pos 8;\nSort [AvgOrdersPerCustomer#698 DESC NULLS LAST], true\n+- Aggregate [RestaurantName#675, timestamp#687-T300000ms], [RestaurantName#675, avg(CustomerOrderCount#697L) AS AvgOrdersPerCustomer#698, timestamp#687-T300000ms]\n   +- SubqueryAlias CustomerOrderCounts\n      +- Aggregate [RestaurantName#675, CustomerID#674, timestamp#687-T300000ms], [RestaurantName#675, CustomerID#674, count(OrderID#673) AS CustomerOrderCount#697L, timestamp#687-T300000ms]\n         +- SubqueryAlias zomato_db\n            +- View (`zomato_db`, [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679, timestamp#687-T300000ms])\n               +- EventTimeWatermark timestamp#687: timestamp, 5 minutes\n                  +- Project [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679, cast(current_timestamp() as timestamp) AS timestamp#687]\n                     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1b8005c8,csv,List(),Some(StructType(StructField(OrderID,IntegerType,true),StructField(CustomerID,IntegerType,true),StructField(RestaurantName,StringType,true),StructField(DishName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true),StructField(OrderDate,DateType,true))),List(),None,Map(header -> true, path -> wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv),None), FileSource[wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv], [OrderID#673, CustomerID#674, RestaurantName#675, DishName#676, Quantity#677, Price#678, OrderDate#679]\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "  \"spark.cosmos.accountEndpoint\": \"https://cosmostestdbtraining.documents.azure.com:443/\",\n",
    "  \"spark.cosmos.accountKey\": \"==\",\n",
    "  \"spark.cosmos.database\": \"test_db\",\n",
    "  \"spark.cosmos.container\": \"container\"\n",
    "}\n",
    "\n",
    "# Configure Catalog Api    \n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", config[\"spark.cosmos.accountEndpoint\"])\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", config[\"spark.cosmos.accountKey\"])\n",
    "\n",
    "def write_to_cosmos(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"cosmos.oltp\") \\\n",
    "        .options(**config) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "# Write the streaming data to Cosmos DB using foreachBatch\n",
    "streaming_query = result_df.writeStream \\\n",
    "    .foreachBatch(write_to_cosmos) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/azuredatabricks/default/checkpoint2\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to finish\n",
    "streaming_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d754762-c292-40a8-980f-58d4a88a81a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-4254577650536249>, line 4\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
       "\u001b[1;32m      2\u001b[0m streaming_query \u001b[38;5;241m=\u001b[39m result_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n",
       "\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(write_to_cosmos) \\\n",
       "\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
       "\u001b[1;32m      5\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
       "\u001b[1;32m      9\u001b[0m streaming_query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py:1637\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n",
       "\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n",
       "\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[0;32m-> 1637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    226\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 9 pos 8;\n",
       "Sort [AvgOrdersPerCustomer#412 DESC NULLS LAST], true\n",
       "+- Aggregate [RestaurantName#389, timestamp#401-T300000ms], [RestaurantName#389, avg(CustomerOrderCount#411L) AS AvgOrdersPerCustomer#412, timestamp#401-T300000ms]\n",
       "   +- SubqueryAlias CustomerOrderCounts\n",
       "      +- Aggregate [RestaurantName#389, CustomerID#388, timestamp#401-T300000ms], [RestaurantName#389, CustomerID#388, count(OrderID#387) AS CustomerOrderCount#411L, timestamp#401-T300000ms]\n",
       "         +- SubqueryAlias zomato_db\n",
       "            +- View (`zomato_db`, [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393, timestamp#401-T300000ms])\n",
       "               +- EventTimeWatermark timestamp#401: timestamp, 5 minutes\n",
       "                  +- Project [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393, cast(current_timestamp() as timestamp) AS timestamp#401]\n",
       "                     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7d4447c0,csv,List(),Some(StructType(StructField(OrderID,IntegerType,true),StructField(CustomerID,IntegerType,true),StructField(RestaurantName,StringType,true),StructField(DishName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true),StructField(OrderDate,DateType,true))),List(),None,Map(header -> true, path -> wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv),None), FileSource[wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv], [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 9 pos 8;\nSort [AvgOrdersPerCustomer#412 DESC NULLS LAST], true\n+- Aggregate [RestaurantName#389, timestamp#401-T300000ms], [RestaurantName#389, avg(CustomerOrderCount#411L) AS AvgOrdersPerCustomer#412, timestamp#401-T300000ms]\n   +- SubqueryAlias CustomerOrderCounts\n      +- Aggregate [RestaurantName#389, CustomerID#388, timestamp#401-T300000ms], [RestaurantName#389, CustomerID#388, count(OrderID#387) AS CustomerOrderCount#411L, timestamp#401-T300000ms]\n         +- SubqueryAlias zomato_db\n            +- View (`zomato_db`, [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393, timestamp#401-T300000ms])\n               +- EventTimeWatermark timestamp#401: timestamp, 5 minutes\n                  +- Project [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393, cast(current_timestamp() as timestamp) AS timestamp#401]\n                     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7d4447c0,csv,List(),Some(StructType(StructField(OrderID,IntegerType,true),StructField(CustomerID,IntegerType,true),StructField(RestaurantName,StringType,true),StructField(DishName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true),StructField(OrderDate,DateType,true))),List(),None,Map(header -> true, path -> wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv),None), FileSource[wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv], [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393]\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 9 pos 8;\nSort [AvgOrdersPerCustomer#412 DESC NULLS LAST], true\n+- Aggregate [RestaurantName#389, timestamp#401-T300000ms], [RestaurantName#389, avg(CustomerOrderCount#411L) AS AvgOrdersPerCustomer#412, timestamp#401-T300000ms]\n   +- SubqueryAlias CustomerOrderCounts\n      +- Aggregate [RestaurantName#389, CustomerID#388, timestamp#401-T300000ms], [RestaurantName#389, CustomerID#388, count(OrderID#387) AS CustomerOrderCount#411L, timestamp#401-T300000ms]\n         +- SubqueryAlias zomato_db\n            +- View (`zomato_db`, [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393, timestamp#401-T300000ms])\n               +- EventTimeWatermark timestamp#401: timestamp, 5 minutes\n                  +- Project [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393, cast(current_timestamp() as timestamp) AS timestamp#401]\n                     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7d4447c0,csv,List(),Some(StructType(StructField(OrderID,IntegerType,true),StructField(CustomerID,IntegerType,true),StructField(RestaurantName,StringType,true),StructField(DishName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true),StructField(OrderDate,DateType,true))),List(),None,Map(header -> true, path -> wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv),None), FileSource[wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv], [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393]\n"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": null,
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m<command-4254577650536249>, line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m streaming_query \u001b[38;5;241m=\u001b[39m result_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(write_to_cosmos) \\\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      5\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      9\u001b[0m streaming_query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py:1637\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
        "\u001b[0;31mAnalysisException\u001b[0m: Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode; line 9 pos 8;\nSort [AvgOrdersPerCustomer#412 DESC NULLS LAST], true\n+- Aggregate [RestaurantName#389, timestamp#401-T300000ms], [RestaurantName#389, avg(CustomerOrderCount#411L) AS AvgOrdersPerCustomer#412, timestamp#401-T300000ms]\n   +- SubqueryAlias CustomerOrderCounts\n      +- Aggregate [RestaurantName#389, CustomerID#388, timestamp#401-T300000ms], [RestaurantName#389, CustomerID#388, count(OrderID#387) AS CustomerOrderCount#411L, timestamp#401-T300000ms]\n         +- SubqueryAlias zomato_db\n            +- View (`zomato_db`, [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393, timestamp#401-T300000ms])\n               +- EventTimeWatermark timestamp#401: timestamp, 5 minutes\n                  +- Project [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393, cast(current_timestamp() as timestamp) AS timestamp#401]\n                     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7d4447c0,csv,List(),Some(StructType(StructField(OrderID,IntegerType,true),StructField(CustomerID,IntegerType,true),StructField(RestaurantName,StringType,true),StructField(DishName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true),StructField(OrderDate,DateType,true))),List(),None,Map(header -> true, path -> wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv),None), FileSource[wasbs://task@azuresynaptraining.blob.core.windows.net/zomato_orders_large.csv], [OrderID#387, CustomerID#388, RestaurantName#389, DishName#390, Quantity#391, Price#392, OrderDate#393]\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "streaming_query = result_df.writeStream \\\n",
    "    .foreachBatch(write_to_cosmos) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"\") \\\n",
    "    .start()\n",
    "\n",
    "streaming_query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL-Structured streaming pipeline using blob",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
